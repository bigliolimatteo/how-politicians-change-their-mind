{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO data analysis esplorativa iniziale \n",
    "# quanti tweet per politico?\n",
    "# quanti like per politico?\n",
    "# quanti retweet?\n",
    "# ...\n",
    "\n",
    "# NOTE that here we also have retweets\n",
    "#for politician in POLITICIANS:\n",
    "#    print(politician + \" \" + str(len([tweet for tweet in date_filtered_data[politician] if \"RT\" not in tweet])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from string import punctuation, digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANT_FIELDS = [\"text\", \"created_at\"]\n",
    "\n",
    "def read_data(input_directory: str):\n",
    "    input_data = dict()\n",
    "\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith(\"json\"):\n",
    "            politician_name = filename.split(\".\")[0]\n",
    "\n",
    "            file_location = os.path.join(input_directory, filename)\n",
    "            file = open(file_location, \"r\")\n",
    "\n",
    "            tweets = json.load(file)[\"tweets\"]\n",
    "\n",
    "            filtered_tweets = [{ key: tweet[key] for key in RELEVANT_FIELDS } for tweet in tweets]\n",
    "\n",
    "            input_data[politician_name] = filtered_tweets\n",
    "            \n",
    "        else: \n",
    "            raise Exception(f\"Input file {filename} has a non supported format.\")\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = read_data(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining cleaning function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove stopwords\n",
    "- Combine tweets that are part1 and part2\n",
    "- DONE Remove tweets after a deadline (e.g. the midnight of the election)\n",
    "- DONE Remove tweets before a deadline (e.g. max 3 months old) -> this is needed because we need to compare similar timeframes\n",
    "- tokenization?\n",
    "- stemming / lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO do we need to remove digits?\n",
    "# TODO is it bad to split tweets into subsentences? -> TODO join multiple splitted tweets ((2/2))\n",
    "# TODO note that there are tweets related to pics that we dont have\n",
    "# TODO how to manage hashtags and citations (#/@)\n",
    "# TODO create a pipeline funciton to use in pandas distributed-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create a pipeline funciton to use in pandas distributed-wise\n",
    "def date_filter(tweet: dict, start_date = datetime(2022,7,22), end_date = datetime(2022,9,25)):\n",
    "    created_at = datetime.strptime(tweet['created_at'], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    return created_at >= start_date and created_at < end_date\n",
    "\n",
    "def remove_links(tweet: dict):\n",
    "    return re.sub(r'http\\S+', '', tweet[\"text\"])\n",
    "\n",
    "def is_retweet(tweet: dict):\n",
    "    return tweet[\"text\"].startswith(\"RT @\")\n",
    "\n",
    "def preclean_tweet(tweet: dict):\n",
    "    if date_filter(tweet) and not is_retweet(tweet):\n",
    "        return remove_links(tweet)  \n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLITICIANS = list(input_data.keys())\n",
    "filtered_data = {politician: list(filter(None, [preclean_tweet(tweet) for tweet in input_data[politician]])) for politician in POLITICIANS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "# Remove stopwords\n",
    "# Lemmatizer/Stemmer\n",
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/billy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "ITALIAN_PUNCTUATION = punctuation + \"â€™\" # TODO add readibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw = set(stopwords.words('italian'))\n",
    "tokenizer = TweetTokenizer()\n",
    "tokenize = lambda text: [word for word in tokenizer.tokenize(text.lower()) \n",
    "                         if word not in ITALIAN_PUNCTUATION and word not in stopw and not word.startswith('http')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = {politician: [tokenize(tweet) for tweet in filtered_data[politician]] for politician in POLITICIANS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmer\n",
    "\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "# PorterStemmer()\n",
    "# SnowballStemmer('italian')\n",
    "# LancasterStemmer()\n",
    "# RegexpStemmer('ita')\n",
    "\n",
    "stem = lambda tokenized_text, stemmer: [stemmer.stem(word.lower()) for word in tokenized_text] \n",
    "\n",
    "stemmed_data = {politician: [stem(tweet, SnowballStemmer('italian')) for tweet in tokenized_data[politician]] for politician in POLITICIANS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweet = list()\n",
    "\n",
    "for tweet, stemmed_tweet in zip(filtered_data[\"bonino\"], stemmed_data[\"bonino\"]):\n",
    "    cleaned_tweet.append([tweet, stemmed_tweet])\n",
    "\n",
    "pd.DataFrame(cleaned_tweet).to_csv(\"TBD/test_stem.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/billy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/billy/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "lemmatize = lambda tokenized_text, lemmatizer: [lemmatizer.lemmatize(word.lower()) for word in tokenized_text] \n",
    "\n",
    "lemmatized_data = {politician: [lemmatize(tweet, WordNetLemmatizer()) for tweet in tokenized_data[politician]] for politician in POLITICIANS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweet = list()\n",
    "\n",
    "for tweet, lemmatized_tweet in zip(filtered_data[\"bonino\"], lemmatized_data[\"bonino\"]):\n",
    "    cleaned_tweet.append([tweet, lemmatized_tweet])\n",
    "\n",
    "pd.DataFrame(cleaned_tweet).to_csv(\"TBD/test_lemma.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   only tf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "#  we firstly use the fit(..) method to fit our estimator to the data\n",
    "tf_vectorizer = TfidfVectorizer(use_idf=False).fit(documents)\n",
    "#  the transform(..) method to transform our count-matrix to a tf-idf representation.\n",
    "X_tf = tf_vectorizer.transform(documents)\n",
    "# Convert to standard matrix\n",
    "X_tf.toarray()\n",
    "\n",
    "\n",
    "#   tfâ€“idf\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "# Convert to standard matrix\n",
    "X_tfidf.toarray()\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# print idf values \n",
    "df_idf = pd.DataFrame(tfidf_vectorizer.idf_\n",
    "                        , index=count_vect.get_feature_names()\n",
    "                        , columns=[\"idf_weights\"])\n",
    "\n",
    "# sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get the first vector out (for the first document) \n",
    "first_vector_tfidf=X_tfidf[0] \n",
    " \n",
    "# place tf-idf values in a pandas data frame \n",
    "df = pd.DataFrame(first_vector_tfidf.T.todense()\n",
    "                    , index=tfidf_vectorizer.get_feature_names()\n",
    "                    , columns=[\"tfidf\"]) \n",
    "\n",
    "df.sort_values(by=[\"tfidf\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweet = list()\n",
    "\n",
    "for tweet in filtered_data[\"bonino\"]:\n",
    "    cleaned_tweet.append([tweet, tokenize(tweet)])\n",
    "\n",
    "pd.DataFrame(cleaned_tweet).to_csv(\"TBD/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IS SPACY USEFUL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation cleaning\n",
    "\n",
    "nlp = spacy.load(\"it_core_news_lg\")\n",
    "\n",
    "corpus = dict()\n",
    "for politician in POLITICIANS:\n",
    "    corpus[politician] = []\n",
    "    for tweet in date_filtered_data[politician]:\n",
    "        for sentence in nlp(tweet).sents:\n",
    "            # TODO add readibility/ do better\n",
    "            sentence_text_w_placeholder = re.sub('\\+Europa', 'SPECIFICPOLITICALPARTYPLACEHOLDER', sentence.text, flags=re.IGNORECASE)\n",
    "            cleaned_sentence_text = \"\".join([x for x in sentence_text_w_placeholder if x not in punctuation and x not in digits])\n",
    "            corpus[politician].append(re.sub('SPECIFICPOLITICALPARTYPLACEHOLDER', '+Europa', cleaned_sentence_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teo/Documents/unimi/how-politician-change-their-mind/.venv/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Tokenization, Lemmatization, StopWords Remover\n",
    "\n",
    "tokenize = lambda text: [x.lemma_.lower() for x in nlp(text) if x.pos_ in ['NOUN', 'PROPN']]\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "X = vectorizer.fit_transform(corpus[\"salvini\"])\n",
    "Xa = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "settembrevotolega    197\n",
      "lega                 181\n",
      "settembre            148\n",
      "italiano              96\n",
      "italia                90\n",
      "                    ... \n",
      "intelligenza           1\n",
      "insicurezza            1\n",
      "insetto                1\n",
      "insegnamento           1\n",
      "ğŸ¥²                      1\n",
      "Length: 1896, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "Xa.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teo/Documents/unimi/how-politician-change-their-mind/.venv/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "tokenize = lambda text: [x.lemma_.lower() for x in nlp(text) if x.pos_ in ['NOUN', 'PROPN']]\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "X = vectorizer.fit_transform(corpus[\"calenda\"])\n",
    "Xa = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drago       137\n",
       "paese       123\n",
       "italia      106\n",
       "pd          106\n",
       "governo     103\n",
       "           ... \n",
       "opinione      1\n",
       "opificio      1\n",
       "distanza      1\n",
       "operaio       1\n",
       "a             1\n",
       "Length: 2408, dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teo/Documents/unimi/how-politician-change-their-mind/.venv/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "secondo      17\n",
       "drago        16\n",
       "campagna     16\n",
       "settembre    14\n",
       "melone       14\n",
       "             ..\n",
       "giugno        1\n",
       "giovedÃ¬       1\n",
       "giovane       1\n",
       "giorgia       1\n",
       "â™‚             1\n",
       "Length: 542, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize = lambda text: [x.lemma_.lower() for x in nlp(text) if x.pos_ in ['NOUN', 'PROPN']]\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "X = vectorizer.fit_transform(corpus[\"renzi\"])\n",
    "Xa = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "Xa.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teo/Documents/unimi/how-politician-change-their-mind/.venv/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sera          58\n",
       "intervista    58\n",
       "movstelle     56\n",
       "grazie        40\n",
       "diretta       32\n",
       "              ..\n",
       "lisola         2\n",
       "linteresse     2\n",
       "linea          2\n",
       "limpegno       2\n",
       "ğŸ“º              2\n",
       "Length: 672, dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize = lambda text: [x.lemma_.lower() for x in nlp(text) if x.pos_ in ['NOUN', 'PROPN']]\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "X = vectorizer.fit_transform(corpus[\"conte\"])\n",
    "Xa = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "Xa.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(corpus: list):\n",
    "    # TODO\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data = clean_data(input_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9eaf728c2940a6692d2e7d7e6ce72533e60deae695939243e176787cb5691bda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
