{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO data analysis esplorativa iniziale \n",
    "# quanti tweet per politico?\n",
    "# quanti like per politico?\n",
    "# quanti retweet?\n",
    "# ...\n",
    "\n",
    "# NOTE that here we also have retweets\n",
    "#for politician in POLITICIANS:\n",
    "#    print(politician + \" \" + str(len([tweet for tweet in date_filtered_data[politician] if \"RT\" not in tweet])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from string import punctuation, digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANT_FIELDS = [\"text\", \"created_at\"]\n",
    "\n",
    "def read_data(input_directory: str):\n",
    "    input_data = dict()\n",
    "\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith(\"json\"):\n",
    "            politician_name = filename.split(\".\")[0]\n",
    "\n",
    "            file_location = os.path.join(input_directory, filename)\n",
    "            file = open(file_location, \"r\")\n",
    "\n",
    "            tweets = json.load(file)[\"tweets\"]\n",
    "\n",
    "            filtered_tweets = [{ key: tweet[key] for key in RELEVANT_FIELDS } for tweet in tweets]\n",
    "\n",
    "            input_data[politician_name] = filtered_tweets\n",
    "            \n",
    "        else: \n",
    "            raise Exception(f\"Input file {filename} has a non supported format.\")\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = read_data(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining cleaning function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove stopwords\n",
    "- Combine tweets that are part1 and part2\n",
    "- DONE Remove tweets after a deadline (e.g. the midnight of the election)\n",
    "- DONE Remove tweets before a deadline (e.g. max 3 months old) -> this is needed because we need to compare similar timeframes\n",
    "- tokenization?\n",
    "- stemming / lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO do we need to remove digits?\n",
    "# TODO is it bad to split tweets into subsentences? -> TODO join multiple splitted tweets ((2/2))\n",
    "# TODO note that there are tweets related to pics that we dont have\n",
    "# TODO how to manage hashtags and citations (#/@)\n",
    "# TODO create a pipeline funciton to use in pandas distributed-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create a pipeline funciton to use in pandas distributed-wise\n",
    "def date_filter(tweet: dict, start_date = datetime(2022,7,22), end_date = datetime(2022,9,25)):\n",
    "    created_at = datetime.strptime(tweet['created_at'], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    return created_at >= start_date and created_at < end_date\n",
    "\n",
    "def remove_links(tweet: dict):\n",
    "    return re.sub(r'http\\S+', '', tweet[\"text\"])\n",
    "\n",
    "def is_retweet(tweet: dict):\n",
    "    return tweet[\"text\"].startswith(\"RT @\")\n",
    "\n",
    "def preclean_tweet(tweet: dict):\n",
    "    if date_filter(tweet) and not is_retweet(tweet):\n",
    "        return remove_links(tweet)  \n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLITICIANS = list(input_data.keys())\n",
    "date_filtered_data = {politician: list(filter(None, [preclean_tweet(tweet) for tweet in input_data[politician]])) for politician in POLITICIANS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation cleaning\n",
    "\n",
    "nlp = spacy.load(\"it_core_news_lg\")\n",
    "\n",
    "corpus = dict()\n",
    "for politician in POLITICIANS:\n",
    "    corpus[politician] = []\n",
    "    for tweet in date_filtered_data[politician]:\n",
    "        for sentence in nlp(tweet).sents:\n",
    "            # TODO add readibility/ do better\n",
    "            sentence_text_w_placeholder = re.sub('\\+Europa', 'SPECIFICPOLITICALPARTYPLACEHOLDER', sentence.text, flags=re.IGNORECASE)\n",
    "            cleaned_sentence_text = \"\".join([x for x in sentence_text_w_placeholder if x not in punctuation and x not in digits])\n",
    "            corpus[politician].append(re.sub('SPECIFICPOLITICALPARTYPLACEHOLDER', '+Europa', cleaned_sentence_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teo/Documents/unimi/how-politician-change-their-mind/.venv/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Tokenization, Lemmatization, StopWords Remover\n",
    "\n",
    "tokenize = lambda text: [x.lemma_.lower() for x in nlp(text) if x.pos_ in ['NOUN', 'PROPN']]\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "X = vectorizer.fit_transform(corpus[\"salvini\"])\n",
    "Xa = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "settembrevotolega    197\n",
      "lega                 181\n",
      "settembre            148\n",
      "italiano              96\n",
      "italia                90\n",
      "                    ... \n",
      "intelligenza           1\n",
      "insicurezza            1\n",
      "insetto                1\n",
      "insegnamento           1\n",
      "ğŸ¥²                      1\n",
      "Length: 1896, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "Xa.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teo/Documents/unimi/how-politician-change-their-mind/.venv/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "tokenize = lambda text: [x.lemma_.lower() for x in nlp(text) if x.pos_ in ['NOUN', 'PROPN']]\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "X = vectorizer.fit_transform(corpus[\"calenda\"])\n",
    "Xa = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drago       137\n",
       "paese       123\n",
       "italia      106\n",
       "pd          106\n",
       "governo     103\n",
       "           ... \n",
       "opinione      1\n",
       "opificio      1\n",
       "distanza      1\n",
       "operaio       1\n",
       "a             1\n",
       "Length: 2408, dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teo/Documents/unimi/how-politician-change-their-mind/.venv/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "secondo      17\n",
       "drago        16\n",
       "campagna     16\n",
       "settembre    14\n",
       "melone       14\n",
       "             ..\n",
       "giugno        1\n",
       "giovedÃ¬       1\n",
       "giovane       1\n",
       "giorgia       1\n",
       "â™‚             1\n",
       "Length: 542, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize = lambda text: [x.lemma_.lower() for x in nlp(text) if x.pos_ in ['NOUN', 'PROPN']]\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "X = vectorizer.fit_transform(corpus[\"renzi\"])\n",
    "Xa = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "Xa.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teo/Documents/unimi/how-politician-change-their-mind/.venv/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sera          58\n",
       "intervista    58\n",
       "movstelle     56\n",
       "grazie        40\n",
       "diretta       32\n",
       "              ..\n",
       "lisola         2\n",
       "linteresse     2\n",
       "linea          2\n",
       "limpegno       2\n",
       "ğŸ“º              2\n",
       "Length: 672, dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize = lambda text: [x.lemma_.lower() for x in nlp(text) if x.pos_ in ['NOUN', 'PROPN']]\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)\n",
    "X = vectorizer.fit_transform(corpus[\"conte\"])\n",
    "Xa = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "Xa.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(corpus: list):\n",
    "    # TODO\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data = clean_data(input_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8be2f95a085d0ad7610f7ce6ffb4128f3c8ce3107f51d8e6715a8724abe2109a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
